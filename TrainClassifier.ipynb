{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix: (20565, 256), Targets: (20565,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "in_dir = \"preprocessed_data\"\n",
    "\n",
    "labels = listdir(in_dir)\n",
    "\n",
    "count = 0\n",
    "for label in labels:\n",
    "    count += len(listdir(join(in_dir, label)))\n",
    "\n",
    "mat = np.zeros((count, 256), dtype=np.int16)\n",
    "targets = np.zeros(count, dtype=np.int8)\n",
    "\n",
    "print(f\"Shape of Matrix: {np.shape(mat)}, Targets: {np.shape(targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'image' -> Done\n",
      "Loading 'text' -> Done\n",
      "Loading 'audio' -> Done\n",
      "Loading 'executable' -> Done\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Loading '{label}' \", end=\"\")\n",
    "    \n",
    "    for filename in listdir(join(in_dir, label)):\n",
    "        with open(join(in_dir, label, filename)) as file:\n",
    "            for line in file.readlines():\n",
    "                tokens = line.strip().split(\":\")\n",
    "                mat[index][int(tokens[0])] = int(tokens[1])\n",
    "        \n",
    "        targets[index] = i\n",
    "        index += 1\n",
    "    \n",
    "    print(\"-> Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data: (14395, 256), Testing Data: (6170, 256)\n",
      "Shape of Training Targets: (14395,), Testing Targets: (6170,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(mat, targets, test_size=0.3, random_state=10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)\n",
    "\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "print(f\"Shape of Training Data: {np.shape(data_train)}, Testing Data: {np.shape(data_test)}\")\n",
    "print(f\"Shape of Training Targets: {np.shape(targets_train)}, Testing Targets: {np.shape(targets_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLPClassifier -> Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(128,64,32,16), alpha=1e-5, activation='relu', solver='adam')\n",
    "\n",
    "print(\"Training MLPClassifier \", end=\"\")\n",
    "clf.fit(data_train, targets_train)\n",
    "print(\"-> Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['image', 'text', 'audio', 'executable']\n",
      "Accuracy: 98.91410048622366\n",
      "Percision: 98.91589481346284\n",
      "Recall: 98.91410048622366\n",
      "F1 Score: 98.91448506509334\n",
      "[[1650    1   19    1]\n",
      " [   0 1572    0    1]\n",
      " [  23    0 1399    4]\n",
      " [   2    4   12 1482]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "predictions = clf.predict(data_test)\n",
    "accuracy = 100.0 * accuracy_score(targets_test, predictions)\n",
    "percision = 100.0 * precision_score(targets_test, predictions, average=\"weighted\")\n",
    "recall = 100.0 * recall_score(targets_test, predictions, average=\"weighted\")\n",
    "f1_score = 100.0 * f1_score(targets_test, predictions, average=\"weighted\")\n",
    "cmat = confusion_matrix(targets_test, predictions)\n",
    "\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Percision: {percision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model\n",
      "Saving Scaler\n",
      "Writing Labels\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "model_dir = \"model\"\n",
    "\n",
    "print(\"Saving Model\")\n",
    "dump(clf, join(model_dir, \"model.dat\"))\n",
    "\n",
    "print(\"Saving Scaler\")\n",
    "dump(scaler, join(model_dir, \"scaler.dat\"))\n",
    "\n",
    "print(\"Writing Labels\")\n",
    "with open(join(model_dir, \"labels.csv\"), \"w\") as file:\n",
    "    file.write(\",\".join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
